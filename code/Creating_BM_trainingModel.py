# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
from os import listdir
from os.path import isfile, join

from sklearn.metrics import log_loss, confusion_matrix
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import scale, MinMaxScaler
from sklearn.linear_model import SGDClassifier
from sklearn.externals import joblib
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from collections import Counter
from sklearn import tree
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB

import numpy as np
import os
import pickle
import re
import time

##################################################################################################################

def hasNumbers(inputString):
    return any(char.isdigit() for char in inputString)

##################################################################################################################

#### Load training ####
Y = pickle.load(open("BMsave/Y_data_asm", "r+b"))
nb_sample = len(Y)

s_word_asm = pickle.load(open("BMsave/set_word_asm", "r+b"))
X_asm = pickle.load(open("BMsave/X_data_asm", "r+b"))
idataVSize = pickle.load(open('BMsave/idataVSize', 'r+b'))
dllset = pickle.load(open('BMsave/dllset', 'r+b'))
dllcollect = pickle.load(open('BMsave/dllcollect', 'r+b'))


dict_s = dict(zip(s_word_asm, range(len(s_word_asm))))
outfile1 = open('BMsave\dict_s_asm', 'w+b')
pickle.dump(dict_s, outfile1)
outfile1.close()


dict_dll = dict(zip(dllset, range(len(dllset))))
outfile1 = open('BMsave\dict_dll', 'w+b')
pickle.dump(dict_dll, outfile1)
outfile1.close()


#data_asm = np.zeros((nb_sample , len(s_word_asm) + len(dllset) + len(idataVSize[0]) ))
data_asm = np.zeros((nb_sample , len(s_word_asm) + len(dllset) + len(idataVSize[0]) ))

for i, cnt in enumerate(X_asm):

    if cnt!=0:
        for el in cnt:
            data_asm[i][dict_s[el]] = cnt[el]

    if dllcollect[i]!=[]:
        for each in dllcollect[i]:
            if each in dict_dll:
                data_asm[i][len(s_word_asm) + dict_dll[each]] = 1

    for j in range(len(idataVSize[0])):
        data_asm[i][len(s_word_asm) + len(dllset) + j] = idataVSize[i][j] #idataVariable size


#===data_asm = np.log(data_asm + 1)==#防止MemoryError
half=len(data_asm)/2
data_asm[:int(half)] = np.log(data_asm[:int(half)] + 1)
data_asm[int(half):] = np.log(data_asm[int(half):] + 1)

min_max_scaler = MinMaxScaler()
data_asm = min_max_scaler.fit_transform(data_asm)


#### Train ####
#clf = SGDClassifier(loss="log", max_iter=100, shuffle=True, n_jobs=2)
clf = RandomForestClassifier(n_estimators=10, criterion='gini')
#clf = tree.DecisionTreeClassifier(criterion='entropy')
#clf = GaussianNB()
#clf = MultinomialNB()
#clf = KNeighborsClassifier(n_neighbors=3)
#clf = SVC(kernel='sigmoid')
#clf = SVC(kernel="linear")
#clf = SVC(kernel="rbf", gamma=0.7)


start = time.clock()  # 计算时间
clf.fit(data_asm, Y)
end = time.clock()  # 计算时间
print('\ntraining time:', end - start)

joblib.dump(clf, "BMsave/clf_model.m")
joblib.dump(min_max_scaler, "BMsave/min_max_scaler.m")
print('Saved',end='\n\n\n')

clf = joblib.load("BMsave/clf_model.m")

r = clf.predict(data_asm)
p = clf.predict_proba(data_asm)

print('loss = ',end='')
print ("%.3f" % log_loss(Y, p))
print('accuracy = ',end='')
print(accuracy_score(Y, r))

cm = confusion_matrix(Y, r)
print(classification_report(Y, r))
print(cm)
print('\n')

'''
#总验证
loss = 0.099
accuracy = 0.96865520728
             precision    recall  f1-score   support

          0       0.99      0.94      0.97      1027
          1       0.94      0.99      0.97       951

avg / total       0.97      0.97      0.97      1978

[[970  57]
 [  5 946]]

'''


def run_cv(X, y, clf):
    # Construct a kfolds object
    kf = KFold(n_splits=5, shuffle=True)
    y_prob = np.zeros((len(y), 2))
    y_pred = np.zeros(len(y))

    # Iterate through folds
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train = y[train_index]

        clf.fit(X_train, y_train)
        y_prob[test_index] = clf.predict_proba(X_test)
        y_pred[test_index] = clf.predict(X_test)

    return y_prob, y_pred


#### See potential score on CV ####
#clf = SGDClassifier(loss="log", max_iter=100, shuffle=True, n_jobs=2)
clf = RandomForestClassifier(n_estimators=10, criterion='gini')
#clf = tree.DecisionTreeClassifier(criterion='entropy')
#clf = KNeighborsClassifier(n_neighbors=3)
#clf = GaussianNB()
#clf = MultinomialNB()
#clf = SVC(kernel='sigmoid')
#clf = SVC(kernel="rbf", gamma=0.7)


prob, pred = run_cv(data_asm, Y, clf)

"""
########################################
prob, pred = run_cv(data_asm, Y, clf)
joblib.dump(prob, "f_result/KNN3_opcode+dll_prob.save")
joblib.dump(pred, "f_result/KNN3_opcode+dll_pred.save")
joblib.dump(Y, "f_result/Y.save")
print("\n\nresult save\n\n\n")
##################################
"""

print('loss = ',end='')
print ("%.3f" % log_loss(Y, prob))
print('accuracy = ',end='')
print(accuracy_score(Y, pred))

cm = confusion_matrix(Y, pred)
print(classification_report(Y, pred))
print(cm)

'''
#交叉验证
loss = 0.199
accuracy = 0.941354903943
             precision    recall  f1-score   support

          0       0.96      0.92      0.94      1027
          1       0.92      0.96      0.94       951

avg / total       0.94      0.94      0.94      1978

[[948  79]
 [ 37 914]]

'''



"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit

# 用sklearn的learning_curve得到training_score和cv_score，使用matplotlib画出learning curve
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1,
                        train_sizes=np.linspace(.1, 1.0, 5), plot=True):

    #画出data在某模型上的learning curve.
    #参数解释
    #----------
    #estimator : 你用的分类器。
    #title : 表格的标题。
    #X : 输入的feature，numpy类型
    #y : 输入的target vector
    #ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点
    #cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)
    #n_jobs : 并行的的任务数(默认1)

    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    if plot:
        plt.figure()
        plt.title(title)
        if ylim is not None:
            plt.ylim(*ylim)
        plt.xlabel(u"train data num")
        plt.ylabel(u"sorce")
        plt.gca().invert_yaxis()
        plt.grid()

        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,
                         alpha=0.1, color="b")
        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std,
                         alpha=0.1, color="r")
        plt.plot(train_sizes, train_scores_mean, 'o-', color="b", label=u"traing sorce")
        plt.plot(train_sizes, test_scores_mean, 'o-', color="r", label=u"cv sorce")

        plt.legend(loc="best")

        plt.draw()
        plt.show()
        plt.gca().invert_yaxis()

    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2
    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])
    return midpoint, diff

clf = SGDClassifier(loss="log", max_iter=100, shuffle=True, n_jobs=2)
#clf = tree.DecisionTreeClassifier(criterion='gini')

cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
plot_learning_curve(clf, u"Learning Curves", data_asm, Y,cv=cv)
"""

"""
loss = 0.569
accuracy = 0.764738292011
             precision    recall  f1-score   support

          0       0.00      0.18      0.00        17
          1       1.00      0.77      0.87     10873

avg / total       1.00      0.76      0.87     10890

[[   3   14]
 [2548 8325]]
"""

"""
#Import Library
#Import other necessary libraries like pandas, numpy...

from sklearn import tree
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create tree object
model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini

# model = tree.DecisionTreeRegressor() for regression

# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)

#Predict Output
predicted= model.predict(x_test)
"""